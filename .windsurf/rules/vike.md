---
trigger: always_on
---

# Next.js Project — LLM & Code Contribution Rule File

> Purpose: Provide a definitive set of rules, guardrails, and workflows for a Next.js codebase that uses LLMs and automated agents. These rules are intended to reduce hallucination, enforce safe edits, maintain code quality, require clear human feedback loops, and produce an auditable `report.md` for every change.

---

## Table of contents

1. Scope & Definitions
2. High-level Principles
3. LLM Interaction Rules
4. Code Edit Rules
5. File Size & Refactor Rules
6. Reporting — `report.md` (template + requirements)
7. Coding Standards & Tooling
8. Testing & Quality Gates
9. CI/CD & Automation
10. Security, Secrets & Privacy
11. PR / Review / Approvals Workflow
12. Incident & Rollback Procedures
13. Enforcement & Exceptions
14. Appendix: Examples & Templates

---

## 1. Scope & Definitions

* **Project scope:** All code in the repository, scripts used by CI/CD, infra-as-code related to the Next.js app, prompt templates, and any agent or LLM orchestration code.
* **Contributor:** Any human or automated system (bot, agent) that modifies repository content.
* **LLM-agent:** Any automated process that composes prompts, edits files, or proposes code changes.
* **Change operation:** A single logical action that results in code edits (an automated run or a human edit). For LLMs this means a single execution of an edit routine.

---

## 2. High-level Principles

1. **Minimize hallucinatory output.** Treat LLM outputs as suggestions — do not accept generated claims or code without verification and tests.
2. **Explicit human confirmation.** LLMs and automated agents must present clarifying questions and obtain explicit human approval for non-trivial changes.
3. **Small, reviewable edits.** A single change operation must not modify more than **2 files**. If a change must touch more than two files, split into multiple well-scoped operations with intermediate reviews.
4. **Auditability & reporting.** Every automated or human change must produce a `report.md` entry explaining intent, changed files, tests added, and validation steps.
5. **Refactor when necessary.** If a file exceeds a configured complexity/size threshold, refactor to smaller modules and include tests for the refactor.

---

## 3. LLM Interaction Rules

### 3.1 Mandatory safeguards

* **No hallucination policy:** LLM responses that assert facts (e.g., external API capabilities, library behavior, dataset contents) must include a confidence flag and a required verification step. The LLM must attach a `VERIFICATION_REQUIRED: true` marker for any factual claim that is not trivially provable by running the code or tests in-repo.
* **Cite sources where possible:** When the LLM references docs, APIs, or external behavior, it must provide the exact documentation URL or local doc file reference in the `report.md` entry.
* **Deterministic prompts:** Store and version prompt templates. Each run must record: prompt version, input, temperature, model name, result id.

### 3.2 Interaction behavior

* **Always ask clarifying questions before proceeding.** If the LLM or agent is given an ambiguous instruction (requirement missing, unclear scope, risky change), it must request clarification and present at least 2 explicit options. The agent must not proceed until human confirmation is recorded in `report.md`.
* **Limit edits per run:** An LLM-run may not edit more than **2 files** in one operation. If more edits are required, the agent should propose a plan (sequence of operations) and ask human approval for the first operation.
* **Human-in-the-loop gating for high-risk changes:** Any change touching auth, payments, secrets, infra, access controls, telemetry, or anything described in the security policy must be explicitly approved by a designated human reviewer (see CODEOWNERS).

### 3.3 Output validation

* **Schema validation for generated data:** If the LLM generates structured data (JSON, API contract), validate it against a JSON Schema before accepting it.
* **Static checks for generated code:** Run linting, type-checking (TS), and unit tests locally (or in CI) on generated code. The LLM must not mark a change as complete unless all local checks passed.

---

## 4. Code Edit Rules

### 4.1 Edit limits and atomicity

* **Max files per operation:** 2 files. This applies to any single automated commit or pull request generated by an agent. Humans should also try to keep to small changes but have a separate guidance (e.g., keep PRs < 400 lines changed when possible).
* **Atomic intent:** Each change must implement a single clear intent (bugfix, feature, refactor). Mixes of intent should be split into separate PRs.

### 4.2 Edit metadata

Every change MUST include the following metadata in the `report.md` and PR description:

* `Author` (human or agent id)
* `Change Type` (bugfix/feature/refactor/docs/chore)
* `Files changed` (list)
* `LLM involvement` (yes/no; if yes: model name, prompt template id, temperature, run id)
* `Tests added/updated` (yes/no + list)

### 4.3 Formatting & dependencies

* **Pre-commit hooks** must run: `prettier`, `eslint --fix`, `pnpm/yarn/npm install` check, `type-check` (if TS), `jest --bail --runInBand --findRelatedTests <files>` for changed files.
* **Dependency edits**: any dependency version changes must be isolated into a separate PR and require security scan results (e.g., `npm audit`/`Snyk`) attached to `report.md`.

---

## 5. File Size & Refactor Rules

### 5.1 Thresholds (configurable)

* **LOC threshold:** If a source file (non-test) exceeds **400 lines** of code (LOC, excluding comments and blank lines), it should be flagged for refactor.
* **Complexity threshold:** If cyclomatic complexity by function/module > 15, refactor.
* **Single module responsibility:** No file should export more than 3 top-level classes/functions that represent unrelated responsibilities.

### 5.2 Refactor guidelines

* If a file crosses thresholds during a change, the contributor must either:

  1. Split the change into a separate *refactor* PR that moves/splits responsibilities into well-named modules and add tests for each new module, **or**
  2. Reduce the change scope to avoid creating a large diff; the refactor must be scheduled separately.
* **Automated refactors:** If an agent proposes a refactor, it must output a refactor plan and map of old-to-new file paths in `report.md`, run the full test suite, and include the refactor PR for human review.

---

## 6. Reporting — `report.md` (REQUIRED)

Every PR (human or agent) MUST include a `report.md` at the repo root or updated report in a `/reports/` folder. The `report.md` documents the change and acts as an audit trail.

### 6.1 `report.md` structure (template)

```md
# Change Report

- **Date:** YYYY-MM-DD
- **Author:** <name|agent-id>
- **PR / Commit:** <link>
- **Change Type:** bugfix/feature/refactor/chore/docs
- **Files Changed:**
  - path/to/file1
  - path/to/file2
- **Why:** Short explanation of the problem and rationale for the change.
- **LLM Involvement:** yes/no
  - If yes:
    - Model: <model name and version>
    - Prompt Template ID: <template id>
    - Temperature: <value>
    - Run ID: <run id if available>
    - Clarifying Qs asked: list of clarifying Qs asked to humans and the answers
- **Verification Steps:** How this change was validated (unit tests, integration tests, local run, manual QA).
- **Tests Added/Updated:** list of test files and brief explanation.
- **Rollout Plan:** feature flag, canary, full rollout
- **Monitoring & Metrics:** which telemetry to watch after deploy (errors, performance, business metrics)
- **Backout Plan:** steps to rollback if something fails
- **Notes & Caveats:** anything reviewers should be aware of
```

### 6.2 Additional report rules

* `report.md` must be updated with CI status (pass/fail) and link to the artifact logs.
* For agent-generated changes, the `Clarifying Qs asked` block must show the exact question text and timestamped human answer.

---

## 7. Coding Standards & Tooling

* **Language:** TypeScript for Next.js app (recommended). If JS is used, strict linting and JSDoc are required.
* **Linters:** ESLint with recommended Next.js rules, Prettier for formatting.
* **Type checking:** `tsc --noEmit` as part of pre-commit and CI.
* **Testing:** Jest + React Testing Library for frontend unit tests. Playwright for end-to-end (E2E) tests when touching pages.
* **Security linters:** `eslint-plugin-security`, dependency scanning (Snyk or GitHub Dependabot).
* **Commit messages:** Conventional Commits enforced by `commitlint`.
* **Branch naming:** `feature/*`, `bugfix/*`, `chore/*`, `hotfix/*`.

---

## 10. Security, Secrets & Privacy

* **Never store secrets in repo.** Use environment variables / secrets manager (Vercel/GCP/AWS). CI runs must use vault-backed secrets.
* **Secrets scanning:** Enable git-secret scanning and block PRs that introduce secrets.
* **Data privacy:** LLM prompts that include user PII must be anonymized or redacted before being sent to any third-party model unless permitted explicitly and logged in `report.md`.
* **High-risk categories:** Any code that interacts with authentication, payments, user data exports, or escalates privileges must have an explicit security review and cannot be auto-approved.

---
